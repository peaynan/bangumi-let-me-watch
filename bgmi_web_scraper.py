import os
import re
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from typing import Optional, List, Tuple

# é…ç½®
BGMI_WEB_URL = "https://bgm.tv/anime/list/{username}/wish?orderby=date&page={page}"
API_UPDATE_URL = "https://api.bgm.tv/v0/users/-/collections/{subject_id}"
USERNAME = os.getenv("BGMI_USERNAME")
API_KEY = os.getenv("BGMI_API_KEY")

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Authorization": f"Bearer {API_KEY}"
}

def format_chinese_date(date_str: str) -> str:
    """æ—¥æœŸæ ¼å¼åŒ–ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    - ä¸­æ–‡æ ¼å¼ï¼šYYYYå¹´MMæœˆDDæ—¥ â†’ YYYY-MM-DD
    - æ•°å­—æ ¼å¼ï¼šYYYY-MM-DD â†’ ç›´æ¥è¿”å›ï¼ˆç¡®ä¿è¡¥é›¶ï¼‰
    """
    # å°è¯•åŒ¹é…ä¸­æ–‡æ ¼å¼ï¼šYYYYå¹´MMæœˆDDæ—¥
    match_cn = re.search(r'(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', date_str)
    if match_cn:
        y, m, d = match_cn.groups()
        return f"{int(y):04d}-{int(m):02d}-{int(d):02d}"

    # å°è¯•åŒ¹é…æ•°å­—æ ¼å¼ï¼šYYYY-MM-DD
    match_num = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_str)
    if match_num:
        y, m, d = match_num.groups()
        return f"{int(y):04d}-{int(m):02d}-{int(d):02d}"  # ç¡®ä¿è¡¥é›¶

    print(f"âš ï¸ æœªè¯†åˆ«çš„æ—¥æœŸæ ¼å¼: {date_str}")
    return ""

def fetch_page(page: int) -> Optional[str]:
    """è·å–ç½‘é¡µå†…å®¹"""
    url = BGMI_WEB_URL.format(username=USERNAME, page=page)
    try:
        response = requests.get(url, headers={"User-Agent": headers["User-Agent"]}, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ ç½‘é¡µè¯·æ±‚å¤±è´¥ (ç¬¬{page}é¡µ): {e}")
        return None

def parse_subjects(html: str) -> List[Tuple[str, str]]:
    """è§£æç½‘é¡µè·å– subject_id å’Œæ ¼å¼åŒ–æ—¥æœŸ"""
    soup = BeautifulSoup(html, 'html.parser')
    subjects = []

    # è·å–æ‰€æœ‰æ¡ç›®å¡ç‰‡
    items = soup.select('#browserItemList li.item')

    for item in items:
        # æå– subject_id
        cover = item.select_one('.subjectCover')
        if not cover:
            continue
        subject_id = cover['href'].split('/')[-1]

        if re.search(r'\D1è¯', item.get_text().strip()):
            print(f"æ¡ç›® {subject_id} ç–‘ä¼¼ä¸ºå‰§åœºç‰ˆï¼Œè·³è¿‡")
            continue

        # æå–å¹¶æ ¼å¼åŒ–æ—¥æœŸ
        date_tag = item.select_one('.info.tip')
        if not date_tag:
            print(f"âš ï¸ æ¡ç›® {subject_id} ç¼ºå°‘æ—¥æœŸæ ‡ç­¾")
            continue

        raw_date = date_tag.get_text().strip()
        formatted_date = format_chinese_date(raw_date)
        if not formatted_date:
            continue

        subjects.append((subject_id, formatted_date))
        print(f"   â†’ æ¡ç›® {subject_id} | {formatted_date}")

    return subjects

def update_to_watching(subject_id: str, date: str, dry_run: bool) -> bool:
    """æ›´æ–°æ¡ç›®çŠ¶æ€å¹¶è¿”å›æ˜¯å¦æˆåŠŸ"""
    today = datetime.now().strftime("%Y-%m-%d")
    if date != today:
        return False

    if dry_run:
        print(f"ğŸš§ [å®‰å…¨æ¨¡å¼] åº”æ›´æ–°æ¡ç›® {subject_id} ä¸ºã€Œåœ¨çœ‹ã€")
        return True

    try:
        response = requests.post(
            API_UPDATE_URL.format(subject_id=subject_id),
            json={"type": 3},
            headers=headers,
            timeout=10
        )
        if response.status_code == 202:
            print(f"âœ… æˆåŠŸæ›´æ–°æ¡ç›® {subject_id} ä¸ºã€Œåœ¨çœ‹ã€")
            return True
        print(f"âŒ æ›´æ–°å¤±è´¥ (HTTP {response.status_code}): {response.text}")
    except Exception as e:
        print(f"âŒ æ›´æ–°å¼‚å¸¸: {e}")
    return False

def main(dry_run: bool = False):
    print(f"\nğŸ¬ å¼€å§‹åŒæ­¥ Bangumiã€Œæƒ³çœ‹ã€åˆ—è¡¨ ({'å®‰å…¨æ¨¡å¼' if dry_run else 'æ­£å¸¸æ¨¡å¼'})")
    print(f"â° å½“å‰æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}")
    page = 1

    while True:
        print(f"\nğŸ“– æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")
        html = fetch_page(page)
        if not html:
            break

        subjects = parse_subjects(html)
        if not subjects:
            print("â¹ï¸ æ²¡æœ‰æ›´å¤šæ¡ç›®")
            break

        for subject_id, date in subjects:
            if date < datetime.now().strftime("%Y-%m-%d"):
                print(f"â¹ï¸ é‡åˆ°æ—©äºä»Šå¤©çš„æ¡ç›® ({date})ï¼Œç»ˆæ­¢ç¿»é¡µ")
                return

            update_to_watching(subject_id, date, dry_run)

        page += 1

    print("\nğŸ›‘ åŒæ­¥å®Œæˆ")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--dry-run", action="store_true", help="å®‰å…¨æ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹ï¼‰")
    args = parser.parse_args()

    if not USERNAME:
        print("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ BGMI_USERNAME")
        exit(1)

    main(dry_run=args.dry_run)
